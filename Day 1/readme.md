# ğŸ§ Linux Fundamentals â€” Complete Study Guide
### Day 1: What is Linux? Kernel, Distributions & Ecosystem

> **Who is this for?** Beginners who want to deeply understand Linux for DevOps, SRE, and Cloud Engineering roles.
> **Learning Style:** Analogy-first, command-driven, deeply explained.

---

## ğŸ“š Table of Contents

1. [Why Learn Linux?](#1-why-learn-linux)
2. [What is Linux?](#2-what-is-linux)
3. [Brief History of Linux](#3-brief-history-of-linux)
4. [The Linux Layer Model](#4-the-linux-layer-model)
5. [The Kernel â€” Deep Dive](#5-the-kernel--deep-dive)
6. [Kernel Space vs. User Space](#6-kernel-space-vs-user-space)
7. [The Shell â€” Deep Dive](#7-the-shell--deep-dive)
8. [Linux Distributions](#8-linux-distributions)
9. [Package Managers â€” Deep Dive](#9-package-managers--deep-dive)
10. [Linux vs. Windows](#10-linux-vs-windows)
11. [The GNU Project & Open Source Philosophy](#11-the-gnu-project--open-source-philosophy)
12. [Common Beginner Mistakes](#12-common-beginner-mistakes)
13. [Hands-On Tasks](#13-hands-on-tasks)
14. [Quick Revision Summary](#14-quick-revision-summary)
15. [Interview Preparation Q&A](#15-interview-preparation-qa)
16. [Glossary](#16-glossary)

---

## 1. Why Learn Linux?

Before diving into definitions, answer the question every beginner has: *"Why should I care?"*

> **"Every time you visit Google, stream Netflix, or use your phone â€” you are touching Linux. It runs underneath almost everything. As a DevOps or Cloud engineer, you will spend your career living inside Linux terminals. Not knowing Linux is like being a chef who doesn't know how a kitchen works."**

### Why Linux Matters for DevOps / SRE / Cloud Engineers

Linux is the foundation of modern infrastructure. Here's how each role depends on it:

- **DevOps:** Linux powers CI/CD pipelines (Jenkins, GitLab) and container platforms (Docker, Kubernetes). Bash scripting automates deployment workflows.
- **SRE (Site Reliability Engineer):** Linux's stability and configurability ensure reliable, scalable systems. SREs use Linux for monitoring (Prometheus), logging, and high-availability setups.
- **Cloud Engineers:** Major cloud platforms (AWS, GCP, Azure) run on Linux instances. Linux proficiency is key for managing VMs, optimizing costs, and configuring cloud services.
- **Automation and Scalability:** Linux's command-line tools and scripting enable automation, while its lightweight nature supports scaling across thousands of servers.

### Key Industry Stats (2025)
- Over **96%** of the top 1 million web servers run Linux
- **100%** of the top 500 supercomputers run Linux
- Over **80%** of cloud workloads run on Linux
- Most cloud-native tools (Kubernetes, Terraform, Docker) are **Linux-native**
- Employers **expect** Linux proficiency for DevOps/SRE/Cloud roles

---

## 2. What is Linux?

Linux is a free, open-source, Unix-like **operating system kernel** created by **Linus Torvalds in 1991**.

It forms the core of many operating systems called "Linux distributions" (distros). Linux is used in servers, desktops, embedded systems, supercomputers, IoT devices, and mobile devices (Android).

### The Engine Analogy ğŸš—

> Think of the **Linux kernel as a car's engine**. The engine is the core component that makes the car run and controls all essential functions. But a car isn't just an engine â€” it needs a body, seats, and a dashboard to be a complete vehicle. That's what a **distribution** provides.

So:
- **Linux Kernel** = the engine
- **Linux Distribution** = the complete car (engine + everything built around it)
- **You, the user** = the driver

**Key points to remember:**
- Linux = the **kernel** (not the whole OS by itself)
- When bundled with GNU tools and software â†’ called a **Linux Distribution**
- Most beginners think "Linux" = the whole thing. Strictly speaking, **Linux = only the kernel**

---

## 3. Brief History of Linux

| Year | Event |
|---|---|
| **1969** | Unix created at AT&T Bell Labs â€” the grandfather of modern OSes |
| **1983** | Richard Stallman starts the **GNU Project** â€” a free, Unix-like OS |
| **1991** | Linus Torvalds releases the first **Linux kernel** (v0.01) as a personal project |
| **1993** | Debian founded â€” one of the oldest Linux distributions |
| **1994** | Linux kernel **1.0** released â€” first stable milestone |
| **2000s** | Linux gains traction in servers, supercomputers, embedded systems |
| **2004** | Ubuntu founded â€” most beginner-friendly distribution |
| **2010s** | Linux powers Android, cloud platforms (AWS, Azure), containers (Docker) |
| **2025** | Linux runs 80%+ of cloud workloads, foundation for AI and IoT |

> **Fun Fact:** Linus Torvalds still oversees Linux kernel development today through the **Linux Kernel Mailing List (LKML)**, ensuring its collaborative community spirit.

---

## 4. The Linux Layer Model

Visualize Linux as a stack of layers, from physical hardware at the bottom to you at the top. Each layer only communicates with the layers directly above and below it.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   YOU (typing commands)                    â”‚  â† You live here
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   User Applications (Chrome, Docker, Vim)  â”‚  â† What you run
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Shell (Bash, Zsh, Fish)                  â”‚  â† Your command interpreter
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   System Utilities (ls, grep, systemctl)   â”‚  â† Tools from GNU Project
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   System Libraries (glibc, OpenSSL)        â”‚  â† Shared code apps borrow
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   LINUX KERNEL                             â”‚  â† The boss of everything
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   HARDWARE (CPU, RAM, Disk, Network)       â”‚  â† The physical machine
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Understanding Each Layer

**(a) Hardware Layer**
The physical components: CPU, RAM, disk drives, network interfaces, etc. The OS interacts with hardware via device drivers in the kernel. You never touch this layer directly.

**(b) Linux Kernel**
The core. Manages system resources directly â€” process scheduling, memory allocation, device communication, file storage, and networking. Everything above it talks to everything below it *through* the kernel.

**(c) System Libraries**
Shared code (like `glibc`) that applications use for common functions. Instead of every app writing its own code to open a file, they all borrow the same library function. This is code reuse at the OS level.

**(d) System Utilities**
Core commands like `ls`, `grep`, `cp`, `mv`, `systemctl`. These are provided by the **GNU Project** (more on that later). They're the everyday tools you use to manage the system.

**(e) Shell**
Your command interpreter. You type commands in human-readable form, the shell converts them into actions (kernel system calls). Bash is the most common.

**(f) User Applications**
End-user programs: web browsers, text editors (Vim), DevOps tools (Docker, Ansible), web servers (Apache, Nginx). They interact with the OS through system calls.

> âš ï¸ **The #1 Misconception:** Most people think "Linux" means everything in this diagram. Strictly speaking, **Linux = only the kernel layer**. Everything else (bash, ls, grep) comes from the **GNU Project**. This is why the technically correct name is **GNU/Linux**.

---

## 5. The Kernel â€” Deep Dive

### The City Government Analogy ğŸ›ï¸

> Imagine a **city**. The city has roads, electricity, and water supply â†’ this is your **hardware** (CPU, RAM, Disk). The **City Government** â†’ this is your **Kernel**. **Citizens** â†’ these are your **applications** (Chrome, Docker, your scripts). **Laws and permits** â†’ these are **system calls** (how citizens request services).
>
> Citizens don't directly dig roads or control the power grid. They submit a request to the government. The government decides how to handle it, allocates resources, and responds. Citizens trust the government to manage shared resources fairly so nobody monopolizes everything.
>
> **That's exactly how the kernel works.** Your apps never touch hardware directly. They ask the kernel. The kernel decides, acts, and responds.

The kernel has **5 core jobs**. Let's understand each one deeply.

---

### 5.1 Process Management â€” The Traffic Controller

**What it means:** Your computer runs hundreds of programs "at the same time." But your CPU can only do one thing at a time per core. The kernel creates the *illusion* of multitasking using a technique called **scheduling**.

> **Analogy:** Think of a single chef (CPU) in a restaurant kitchen. Customers (processes) all want their food simultaneously. The chef rapidly switches between dishes â€” stir this for 2 seconds, flip that for 1 second â€” so fast that every table feels like they're being served at the same time. The kernel is that chef, and scheduling is the method it uses to decide who gets CPU time, for how long, and in what order.

**What a Process Actually Is:**

When you run any program â€” say `ls` or Chrome â€” the kernel creates a **process**. A process is a running instance of a program. It has:
- Its own **memory space** (other processes can't read it)
- A unique ID called **PID (Process ID)**
- A state: running, sleeping, stopped, or zombie

```bash
# See all running processes right now
ps aux

# Flags explained:
# a = show processes from ALL users (not just yours)
# u = show the USER who owns each process
# x = include processes NOT attached to a terminal (background services)

# See processes in real-time (like Windows Task Manager)
top

# Better, more readable version of top
htop   # Install first with: sudo apt install htop

# Find a specific process by name
ps aux | grep nginx

# Get the PID of a running process
pidof nginx

# Kill a process gently (asks it to stop gracefully)
kill 1234

# Force kill â€” kernel immediately and forcefully destroys the process
kill -9 1234
```

> The `-9` sends **SIGKILL** â€” a signal the kernel sends to a process. Signal 9 (SIGKILL) **cannot be ignored**. The kernel forcefully destroys the process. Use it only when a regular `kill` doesn't work.

---

### 5.2 Memory Management â€” The Hotel Manager

**What it means:** Every running program needs RAM. The kernel decides who gets how much, prevents programs from stealing each other's memory, and manages what happens when RAM fills up.

> **Analogy:** Imagine a hotel (RAM) with 1000 rooms. The kernel is the hotel manager. When Chrome checks in, it gets rooms 1â€“200. When your terminal checks in, it gets rooms 201â€“210. Each guest only has a key to their own rooms â€” **they cannot enter other guests' rooms.** If the hotel fills up, the manager moves less-active guests to a waiting area (swap space on disk) to free up rooms for active guests.

**Virtual Memory â€” The Mind-Bending Part:**

Each process thinks it has the **entire memory to itself**. The kernel creates this illusion using **virtual memory**. Process A's "address 1000" and Process B's "address 1000" are completely different physical locations in RAM. The kernel maintains a translation table mapping each process's virtual addresses to real physical addresses.

This means:
- Processes are isolated from each other (security)
- A process can't accidentally (or maliciously) read another process's data
- The OS can manage memory efficiently behind the scenes

```bash
# See memory usage in human-readable format
free -h

# Output breakdown:
#               total    used    free   shared  buff/cache   available
# Mem:           7.7G    2.1G    3.2G    234M       2.4G       5.1G
# Swap:          2.0G    0.0G    2.0G

# total      = total RAM physically installed in the machine
# used       = RAM currently in use by processes
# free       = RAM that is completely unused and idle
# buff/cache = RAM kernel is using to cache disk reads (speeds things up)
#              The kernel will give this back to apps if they need it
# available  = what new programs can actually use (free + reclaimable cache)
# Swap       = disk space acting as overflow RAM (much slower than RAM, but
#              prevents out-of-memory crashes)

# See the top 10 most memory-hungry processes
ps aux --sort=-%mem | head -10
```

---

### 5.3 File System Management â€” The Library Librarian

**What it means:** Everything in Linux is treated as a file â€” documents, devices, network connections, even processes. The kernel manages how data is stored, organized, retrieved, and protected on disk.

> **Analogy:** Think of the kernel as a **master librarian** managing an enormous library. Every book (file) has a unique location code (path like `/home/user/document.txt`). When you want a book, you ask the librarian â€” you don't go hunting through shelves yourself. The librarian knows exactly where everything is, who's allowed to access what, and ensures two people don't try to write in the same book simultaneously.

**The Linux File System Hierarchy:**

```
/                    â† Root â€” the top of everything (the entire library)
â”œâ”€â”€ bin/             â† Essential commands (ls, cp, bash) needed to boot
â”œâ”€â”€ etc/             â† Configuration files â€” system-wide settings
â”œâ”€â”€ home/            â† Users' personal directories
â”‚   â””â”€â”€ yourname/    â† Your home folder, also written as ~
â”œâ”€â”€ var/             â† Variable data â€” things that change (logs, databases)
â”‚   â””â”€â”€ log/         â† System logs live here
â”œâ”€â”€ tmp/             â† Temporary files â€” cleared automatically on reboot
â”œâ”€â”€ usr/             â† User programs, libraries, documentation
â”‚   â””â”€â”€ bin/         â† Most commands you use (nginx, python, etc.)
â”œâ”€â”€ proc/            â† Virtual filesystem â€” kernel exposes live process info here
â”œâ”€â”€ dev/             â† Device files â€” your hard drive appears as /dev/sda
â””â”€â”€ sys/             â† Virtual filesystem â€” hardware and kernel info
```

**The Mind-Bending Fact â€” Everything is a File:**

In Linux, **everything** is represented as a file â€” including hardware, running processes, and network connections. This "everything is a file" philosophy is what makes Linux so powerful for scripting and automation.

```bash
# Your hard drive is a file
ls /dev/sda

# Your CPU information is a file you can read
cat /proc/cpuinfo

# Your RAM information is a file
cat /proc/meminfo

# Running processes are folders inside /proc
ls /proc/            # Each numbered folder = one process (its PID number)
ls /proc/1/          # Process 1 (systemd) is fully exposed as files
cat /proc/1/status   # Read status of PID 1
```

---

### 5.4 Device Drivers â€” The Universal Translator

**What it means:** Hardware speaks its own language. A Logitech mouse communicates differently than a Samsung SSD. Device drivers are translators that let the kernel speak to each piece of hardware in its native language.

> **Analogy:** Imagine you're a UN diplomat (the kernel) who needs to talk to ambassadors from 50 countries (hardware devices). Each ambassador speaks a different language. You have personal interpreters (drivers) â€” one per country. You speak to your interpreter in English, they translate to the ambassador, and bring the response back in English. **You never have to learn 50 languages yourself** â€” that's the driver's job.

When you plug in a USB drive, the kernel finds the right driver for it, and suddenly `/dev/sdb` appears on your system â€” the drive is available as a file.

```bash
# See all hardware devices the kernel recognizes
lspci          # PCI devices (graphics cards, network cards)
lsusb          # USB devices
lsblk          # Block devices (disks, partitions)

# See loaded kernel modules (currently active drivers)
lsmod

# See kernel messages â€” including when drivers load at boot
dmesg | head -50

# Find hardware errors
dmesg | grep -i error
```

---

### 5.5 Network Management â€” The Post Office

**What it means:** The kernel manages all network communication â€” sending and receiving data packets, managing connections, enforcing firewall rules.

> **Analogy:** The kernel runs a **post office**. Every data packet is a letter with a destination address (IP address) and a mailbox number (port). The kernel sorts incoming mail to the right application and sends outgoing mail to the right destination. Port 80 is the mailbox for web traffic, port 22 for SSH, etc.

```bash
# See your network interfaces and IP addresses
ip addr show          # Modern way (always works)
ifconfig              # Older way (install: sudo apt install net-tools)

# See active network connections and listening ports
ss -tulpn
# -t = show TCP connections
# -u = show UDP connections
# -l = show only LISTENING ports (servers waiting for connections)
# -p = show which process owns each connection
# -n = show port numbers instead of service names (e.g., 22 not 'ssh')

# See your routing table â€” how traffic is directed
ip route show
```

---

### 5.6 How the Kernel Boots

What actually happens from the moment you press the power button to when you see the login prompt:

```
Power Button Pressed
        â†“
BIOS/UEFI runs (firmware â€” checks hardware exists and is functional)
        â†“
Bootloader loads (GRUB â€” Grand Unified Bootloader â€” stored on disk)
        â†“
GRUB loads the Linux kernel from disk into RAM
        â†“
Kernel initializes:
  - Sets up memory management
  - Detects hardware (CPU, RAM, disks)
  - Loads device drivers
  - Mounts the root filesystem
        â†“
Kernel starts the FIRST process: PID 1 (systemd â€” the init system)
        â†“
systemd reads its configuration and starts all services:
  - Networking
  - Logging
  - SSH server
  - etc.
        â†“
Login prompt appears
```

```bash
# Check how long the system has been running
uptime

# Analyze how long each boot phase took
systemd-analyze

# See which services slowed down the boot
systemd-analyze blame
```

---

## 6. Kernel Space vs. User Space

This is a concept many beginners skip â€” but it explains **why Linux is so stable and secure**, and it's essential for understanding containers.

### The High-Security Research Facility Analogy ğŸ”’

> Picture a **nuclear research facility**:
> - **The Secure Inner Lab** = Kernel Space
> - **The Public Reception Area** = User Space
> - **Scientists with full clearance** = The Kernel
> - **Visitors and contractors** = Your Applications
> - **The Request Window** = System Calls
>
> Visitors (apps) **cannot** walk into the secure lab. They submit a request form at the reception window (system call). Scientists (kernel) review it, do the work inside the secure lab, and pass the results back through the window.
>
> Why this strict separation? Because **one reckless visitor should never be able to contaminate the entire lab.**

### Why This Separation Was Invented

In the early days of computing, programs could directly access hardware. One buggy program could corrupt memory used by another program, crash the entire system, or steal sensitive data from other processes.

The kernel/user space separation solves this with **hardware enforcement**. The CPU itself has privilege levels called **rings**:

```
Ring 0 â€” Kernel Mode    â† Full hardware access, no restrictions whatsoever
Ring 1 â€” (rarely used in modern systems)
Ring 2 â€” (rarely used in modern systems)
Ring 3 â€” User Mode      â† Restricted, cannot touch hardware directly
```

Modern systems use only **Ring 0** (kernel) and **Ring 3** (user space). This is **enforced by the CPU hardware itself** â€” it's not just a software rule that could be bypassed.

| Zone | Who lives here | What they can do |
|---|---|---|
| **Kernel Space** | The Linux kernel | Direct hardware access, full memory control, everything |
| **User Space** | All apps (Chrome, Docker, bash, your scripts) | Request services via system calls only â€” cannot touch hardware |

> âš ï¸ **Key insight for beginners:** When a user application crashes, it crashes in **user space**. Because of this separation, the kernel keeps running. The kernel is only affected by its own bugs (very rare). This is why a crashed app doesn't bring down your whole system.

---

### 6.2 System Calls â€” The Only Bridge

A **system call** is the only legal way for user space to request kernel services. Think of it as the "request window" in the research facility analogy.

**What happens during a system call, step by step:**

1. App prepares the request (puts arguments in CPU registers)
2. App executes a special CPU instruction (`syscall` on x86-64)
3. **CPU hardware switches from Ring 3 â†’ Ring 0** (this is a real, physical CPU mode change)
4. Kernel receives the request, validates it, and handles it
5. Kernel puts the result into a CPU register
6. **CPU hardware switches back Ring 0 â†’ Ring 3**
7. App reads the result and continues execution

This entire round trip takes **nanoseconds** but happens **millions of times per second** on a busy system.

**Common System Calls You Trigger Without Knowing:**

| Action You Take | System Call | What the Kernel Does |
|---|---|---|
| Open a file | `open()` | Checks permissions, returns a file descriptor number |
| Read a file | `read()` | Fetches data from disk, puts it in app's memory |
| Write to a file | `write()` | Takes data from app memory, writes it to disk |
| Create a new process | `fork()` | Makes an exact copy of the current process |
| Run a program | `execve()` | Loads a new program into memory, replacing the current one |
| Allocate memory | `mmap()` | Allocates virtual memory pages for the app |
| Open a network connection | `connect()` | Initiates TCP handshake with a remote server |

```bash
# Install strace â€” a tool that intercepts and shows system calls in real time
sudo apt install strace

# Watch ALL system calls that ls makes when you run it
strace ls 2>&1 | less

# You'll see output like this:
# execve("/bin/ls", ["ls"], ...) = 0
# openat(AT_FDCWD, ".", O_RDONLY|O_DIRECTORY) = 3
# getdents64(3, ...) = 240
# write(1, "file1.txt  file2.txt\n", 21) = 21
# exit_group(0)
# Every single line is one system call. This is the real "ls" under the hood.

# Summary view â€” count and time of each system call type
strace -c ls /tmp
```

---

### 6.3 Why This Matters for DevOps â€” The Container Connection

This is where kernel/user space knowledge becomes **directly valuable in your career**.

**Virtual Machines (VMs) â€” The Old Way:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your Application                        â”‚
â”‚  Guest OS (e.g., Ubuntu)  â† Full OS      â”‚
â”‚  Guest Kernel             â† Its own      â”‚
â”‚  Hypervisor (VMware/KVM)  â† Emulates HW  â”‚
â”‚  Host OS + Host Kernel                   â”‚
â”‚  Physical Hardware                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Containers (Docker) â€” The Modern Way:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Container A     â”‚  Container B          â”‚
â”‚  Your App        â”‚  Your App             â”‚
â”‚  Libs/Deps       â”‚  Libs/Deps            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Host Kernel  â† SHARED by ALL containers â”‚
â”‚  Physical Hardware                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Containers share the host kernel** â€” they run in **user space** on the host machine, isolated from each other using two kernel features:

- **cgroups (Control Groups):** Limit how much CPU, RAM, and I/O each container can use
- **namespaces:** Make each container think it's alone on the system â€” it has its own view of processes, network, filesystem, and users

**Why containers are so much lighter than VMs:**
- No duplicate kernel
- No hypervisor overhead
- No full OS boot sequence
- Starting a container = starting a process. Starting a VM = booting a full OS.

> This is also why you **cannot** run a Windows container on a Linux kernel. Containers share the host kernel â€” and a Windows app needs a Windows kernel underneath it.

```bash
# See Linux namespaces in action â€” your current process's namespaces
ls /proc/self/ns/

# See cgroup information for your processes
cat /proc/self/cgroup
```

---

## 7. The Shell â€” Deep Dive

### The Personal Assistant Analogy ğŸ’¼

> You're the CEO (you). The kernel is the factory floor. You don't walk down to the factory and personally operate machinery â€” you have a **Personal Assistant** (the Shell).
>
> You tell your assistant: *"Get me the list of files in the Documents folder."*
>
> Your assistant:
> 1. Understands your request
> 2. Knows who to contact (which kernel system call to use)
> 3. Goes and gets it done
> 4. Brings back the result in a readable format
>
> The shell is that personal assistant. It translates your human-readable commands into kernel system calls.

### 7.1 Types of Shells

| Shell | Known For | When to Use |
|---|---|---|
| **Bash** | Default on nearly all Linux systems, universally available | **Master this first** â€” it's everywhere in DevOps |
| **Zsh** | Better autocomplete, plugins, used by default on macOS | Personal preference for power users |
| **Fish** | Very beginner-friendly, colorful, auto-suggestions | Great for beginners who want a nicer terminal |
| **Dash** | Extremely minimal and fast | Used by system scripts, not interactive use |

**For DevOps/SRE/Cloud:** Master **Bash** first. Every server you'll ever touch will have Bash. Scripts are written in Bash. CI/CD systems use Bash. It's non-negotiable.

---

### 7.2 What Happens When You Type a Command â€” Full Deep Dive

Let's trace `ls -la /home` from the moment you press Enter to the moment you see output. This is one of the most important things to understand.

**Step 1 â€” You type and press Enter**

Your terminal emulator (like GNOME Terminal, iTerm2, or Windows Terminal) captures your keystrokes and sends the string `ls -la /home` to the shell process running inside it.

**Step 2 â€” Shell reads and parses the input**

Bash receives `ls -la /home` and **tokenizes** it â€” breaks it into meaningful parts:
- `ls` = the command to run
- `-la` = flags/options modifying the command's behavior
- `/home` = the argument (what to run the command on)

**Step 3 â€” Shell figures out what `ls` is**

Bash doesn't just blindly run whatever you type. It checks in this **exact order**:

1. **Alias?** Is `ls` defined as an alias? (e.g., `alias ls='ls --color=auto'`)
2. **Function?** Is `ls` a shell function you've defined?
3. **Builtin?** Is `ls` a built-in shell command? (like `cd`, `echo`, `export`, `pwd`)
4. **External program in $PATH?** Search each directory in `$PATH` left to right

```bash
# Check what a command actually is
type ls        # Output: ls is aliased to 'ls --color=auto'
type cd        # Output: cd is a shell builtin
type nginx     # Output: nginx is /usr/sbin/nginx

# Find the full path of an external command
which ls       # Output: /usr/bin/ls
which python3  # Output: /usr/bin/python3

# See your PATH â€” the directories bash searches for commands
echo $PATH
# /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
# Bash searches each directory from LEFT to RIGHT until it finds the command
```

**Step 4 â€” Shell creates a child process using fork()**

Here's where things get fascinating. Bash doesn't run `ls` directly inside itself. Instead:

```
Bash Process (PID 1234) â€” the original shell
        â†“
    fork() system call
    (creates an exact copy of the bash process)
        â†“
Child Process (PID 1235) â€” identical copy of bash
        â†“
    execve("/usr/bin/ls", ["-la", "/home"]) system call
    (REPLACES the child process's memory with the ls program)
        â†“
Child Process (PID 1235) â€” now running ls (not bash anymore)

Meanwhile: Original Bash (PID 1234) calls wait() and pauses until ls finishes
```

This **fork-then-exec** pattern is how Linux creates every single process. It seems inefficient (why copy bash just to replace it?) but it's actually very clever â€” the copy inherits all of bash's open files, environment variables, and settings before being replaced.

**Step 5 â€” ls runs and makes system calls**

Now `ls` is running in user space. To list directory contents, it makes these system calls:

```
openat(AT_FDCWD, "/home", O_RDONLY)   â† Ask kernel to open /home directory
getdents64(3, ...)                     â† Ask kernel to read directory entries
stat("/home/user1", ...)               â† Ask kernel for metadata (size, permissions, dates)
stat("/home/user2", ...)               â† Same for each entry
write(1, "total 8\ndrwxr-xr-x...", .) â† Ask kernel to write output to terminal
```

Every single operation that touches hardware (disk, screen) goes through a system call.

**Step 6 â€” Output reaches your terminal**

`ls` writes to **stdout** (file descriptor 1). Your terminal is connected to this file descriptor. So when `ls` calls `write(1, ...)`, the kernel routes that output to your terminal screen and you see the file listing.

**Step 7 â€” Process exits, bash regains control**

`ls` calls `exit(0)` â€” the `0` means "I finished successfully" (non-zero = error). The kernel destroys the child process. Bash's `wait()` system call returns. Bash displays the prompt again, ready for your next command.

**Full Flow Diagram:**
```
You type: ls -la /home  [press Enter]
          â†“
Bash parses: command="ls", flags="-la", argument="/home"
          â†“
Bash checks: alias? â†’ yes â†’ ls='ls --color=auto'
Bash checks PATH: finds /usr/bin/ls
          â†“
fork() â†’ Child process created (copy of bash)
          â†“
execve("/usr/bin/ls", ["-la", "/home"]) â†’ Child becomes ls
          â†“
ls opens /home (openat system call â†’ kernel)
ls reads entries (getdents64 system call â†’ kernel)
ls gets file info (stat system call â†’ kernel)
ls formats output
          â†“
ls writes to stdout (write system call â†’ kernel â†’ your terminal)
          â†“
exit(0) â†’ child process destroyed
          â†“
Bash's wait() returns â†’ bash shows prompt again
```

---

### 7.3 The Three Standard Streams

Every process in Linux is born with three standard communication channels already open and connected:

| Stream | Number | Default Connection | Purpose |
|---|---|---|---|
| **stdin** | 0 | Your keyboard | Input TO the program |
| **stdout** | 1 | Your terminal screen | Normal output FROM the program |
| **stderr** | 2 | Your terminal screen | Error messages (separate from normal output) |

The reason stderr exists separately from stdout is important: **you can redirect errors to a log file while still seeing normal output on screen** (or vice versa). This is crucial for scripts.

```bash
# Redirect stdout to a file (creates/overwrites the file)
ls /home > output.txt

# Redirect stdout to a file (APPENDS â€” doesn't overwrite)
ls /home >> output.txt

# Redirect stderr (stream 2) to a file
ls /nonexistent_folder 2> errors.txt

# Redirect BOTH stdout AND stderr to the same file
ls /home /nonexistent 2>&1 | less
# 2>&1 means: "send stderr (2) to wherever stdout (1) is going"

# Redirect both to a file directly
ls /home /nonexistent > all_output.txt 2>&1

# The black hole â€” discard output entirely
ls /home > /dev/null           # Discard stdout
ls /home 2> /dev/null          # Discard stderr
ls /home > /dev/null 2>&1      # Discard both

# THE PIPE | â€” the most powerful Linux concept
# Sends stdout of left command as stdin of right command
ls /home | grep "user"

# Real-world pipe examples:
ps aux --sort=-%mem | head -6           # Top 5 memory-hungry processes
ps aux | wc -l                          # Count total running processes
grep -r "Port 22" /etc/ 2>/dev/null     # Find SSH config, suppress errors
ls -lt /var/log | head -10              # 10 most recently changed log files
cat /var/log/syslog | grep "error" | wc -l  # Count errors in syslog
```

> **The pipe `|` is one of the most powerful concepts in Linux.** The Unix philosophy is: build small tools that do one thing well, then chain them together with pipes. `ps` lists processes, `sort` sorts them, `head` takes the top N â€” each tool is simple, but together they're incredibly powerful.

---

### 7.4 Shell Variables and Environment

```bash
# Create a local variable (only in current shell session)
NAME="Linux"
echo $NAME              # Output: Linux

# Variable naming rules: UPPERCASE by convention, no spaces around =
MY_VAR="hello"          # Correct
MY_VAR = "hello"        # WRONG â€” spaces break it, bash thinks = is a command

# Environment variables â€” exported to child processes
export DATABASE_URL="postgres://localhost/mydb"
export API_KEY="abc123"

# Now any program you run from this shell can read DATABASE_URL

# See all environment variables currently set
env
printenv

# See one specific variable
printenv HOME
echo $HOME

# Important built-in environment variables you must know:
echo $HOME      # Your home directory: /home/yourname
echo $USER      # Your username: yourname
echo $SHELL     # Your current shell: /bin/bash
echo $PATH      # Directories bash searches for commands
echo $PS1       # Your prompt format (customize this to change your prompt)
echo $$         # PID of the current shell process
echo $?         # Exit code of the LAST command (0 = success, non-zero = error)
```

```bash
# Using $? to check if a command succeeded
ls /home
echo $?     # Output: 0 (success)

ls /nonexistent
echo $?     # Output: 2 (error â€” file not found)

# In scripts, you use $? to handle errors:
sudo apt install nginx
if [ $? -eq 0 ]; then
    echo "nginx installed successfully"
else
    echo "installation failed!"
fi
```

> **Why this matters for DevOps:** Applications are configured through environment variables. Docker containers receive configuration this way (`docker run -e DATABASE_URL=...`). CI/CD pipelines pass secrets as environment variables. Kubernetes passes config via environment variables. This pattern is **fundamental** to modern DevOps â€” master it.

---

### 7.5 Shell Expansions

Bash transforms your command **before** executing it. Understanding this prevents many confusing bugs:

```bash
# Glob expansion â€” * matches any characters, ? matches one character
ls *.txt               # List all files ending in .txt
ls file?.txt           # Matches file1.txt, file2.txt but not file10.txt
ls /var/log/*.log      # All .log files in /var/log

# Brace expansion â€” generates multiple strings
echo {1..5}            # Output: 1 2 3 4 5
echo {a..e}            # Output: a b c d e
mkdir dir{1,2,3}       # Creates: dir1  dir2  dir3
cp file.txt{,.backup}  # Copies file.txt to file.txt.backup (neat trick!)

# Command substitution â€” run a command and use its output inline
echo "Today is $(date)"
echo "Kernel version: $(uname -r)"
CURRENT_USER=$(whoami)    # Store command output in a variable

# Arithmetic expansion â€” do math in bash
echo $((10 + 5))           # Output: 15
echo $((100 / 4))          # Output: 25
echo $((2 ** 8))           # Output: 256 (2 to the power of 8)
TOTAL=$((5 * 12))          # Store result in variable

# Variable expansion with a default value (very useful in scripts)
echo ${NAME:-"default_value"}   # If NAME is unset or empty, use "default_value"
echo ${PORT:-8080}              # Use 8080 if PORT isn't set

# Order of expansion â€” bash does all this BEFORE running the command:
# 1. Brace expansion: {1..3}
# 2. Tilde expansion: ~ â†’ /home/yourname
# 3. Variable expansion: $HOME â†’ /home/yourname
# 4. Command substitution: $(date) â†’ Mon Feb 18 2026
# 5. Arithmetic expansion: $((5+5)) â†’ 10
# 6. Glob expansion: *.txt â†’ file1.txt file2.txt
```

---

## 8. Linux Distributions

### The Smartphone Analogy ğŸ“±

> Think of different smartphone brands. Samsung, Google Pixel, and OnePlus all use the same ARM processor architecture (the kernel), but they build completely different phones around it â€” different UIs, pre-installed apps, and target users. That's exactly what Linux distributions are.

Different car manufacturers use similar engines (the kernel) but build different vehicles for different purposes:
- **Toyota Camry** = Ubuntu (reliable, popular, beginner-friendly)
- **Alpine/Racing Bicycle** = Alpine Linux (ultralight, minimal, perfect for containers)
- **Corporate Fleet Vehicle** = RHEL (fully supported, enterprise-grade)
- **DIY Kit Car** = Arch Linux (build it exactly how you want)

### 8.1 Popular Distributions by Category

| Category | Distributions | Best For |
|---|---|---|
| **Beginner-friendly** | Ubuntu, Linux Mint, Fedora | Desktop users new to Linux |
| **Enterprise/Server** | RHEL, CentOS Stream, Ubuntu Server | Production servers, cloud VMs |
| **Security-focused** | Kali Linux, Parrot Security OS | Penetration testing, security research |
| **Lightweight/Container** | Alpine Linux, Raspberry Pi OS | Docker containers (~5MB image!), IoT |
| **Advanced/Custom** | Arch Linux, Gentoo | Experienced users who want full control |

**For DevOps/SRE/Cloud Engineers specifically:**
- **Ubuntu Server** â€” Most popular for cloud, huge community, great documentation
- **Amazon Linux** â€” AWS's custom distro, optimized for EC2 instances
- **Alpine Linux** â€” Tiny (~5MB), the most popular base image for Docker containers
- **RHEL/CentOS Stream** â€” Enterprise standard, long-term support, common in large companies

---

### 8.2 How Distros Are Related â€” The Family Tree

Understanding family relationships helps because distros in the same family share the same package manager, similar file locations, and similar configuration patterns. Learn one, and you mostly know the whole family.

```
Unix (1969, AT&T)
â”œâ”€â”€ BSD (Berkeley Software Distribution)
â”‚   â””â”€â”€ macOS (Apple's Darwin kernel â€” this is why mac terminal feels like Linux)
â””â”€â”€ GNU/Linux
    â”œâ”€â”€ Debian (1993) â€” community-driven, extremely stable
    â”‚   â”œâ”€â”€ Ubuntu (2004) â€” most popular for beginners, cloud, and servers
    â”‚   â”‚   â”œâ”€â”€ Linux Mint â€” even more beginner-friendly, Windows-like feel
    â”‚   â”‚   â””â”€â”€ Pop!_OS â€” popular with developers
    â”‚   â””â”€â”€ Kali Linux â€” security and penetration testing
    â”‚
    â”œâ”€â”€ Red Hat (1994) â€” enterprise-focused
    â”‚   â”œâ”€â”€ RHEL (Red Hat Enterprise Linux) â€” paid enterprise, long-term support
    â”‚   â”œâ”€â”€ CentOS Stream â€” free RHEL upstream (what RHEL will become)
    â”‚   â”œâ”€â”€ Fedora â€” cutting edge, community, tests features before RHEL
    â”‚   â””â”€â”€ Amazon Linux â€” AWS's customized version for EC2
    â”‚
    â””â”€â”€ Arch Linux (2002) â€” for advanced users who want total control
        â””â”€â”€ Manjaro â€” more beginner-friendly Arch variant
```

**Family rules:**
- **Debian family** â†’ uses `apt` package manager, `.deb` packages
- **Red Hat family** â†’ uses `dnf`/`yum` package manager, `.rpm` packages
- **Arch family** â†’ uses `pacman` package manager

---

### 8.3 What Makes a Distribution Different?

When someone creates a new distribution, they make choices about:

1. **Package Manager** â€” how users install/remove/update software
2. **Init System** â€” how services start at boot (almost all use `systemd` now)
3. **Default Software** â€” what comes pre-installed out of the box
4. **Release Cycle** â€” how often updates arrive:
   - **Fixed release** (Ubuntu LTS every 2 years, supported for 5 years) = stable, predictable
   - **Rolling release** (Arch Linux) = always latest, but can occasionally break
5. **Support Period** â€” how long old versions receive security updates
6. **Target Audience** â€” desktop users? enterprise servers? containers? embedded devices?

```bash
# ALWAYS check what distro you're on before doing anything
# This tells you which package manager to use and what commands work
cat /etc/os-release

# Example output on Ubuntu 22.04:
# NAME="Ubuntu"
# VERSION="22.04.3 LTS (Jammy Jellyfish)"
# ID=ubuntu
# ID_LIKE=debian          â† tells you it's Debian-based (use apt!)
# VERSION_ID="22.04"
# HOME_URL="https://www.ubuntu.com/"

# Example output on Amazon Linux 2:
# NAME="Amazon Linux"
# VERSION="2"
# ID="amzn"
# ID_LIKE="centos rhel fedora"   â† tells you it's Red Hat-based (use yum/dnf!)

# Check kernel version
uname -r        # e.g., 5.15.0-91-generic
uname -a        # All info: kernel, hostname, architecture, date
```

---

## 9. Package Managers â€” Deep Dive

### 9.1 What Problem Do Package Managers Solve?

**Without a package manager** â€” installing software on Linux meant:

1. Find the source code somewhere online
2. Download a tarball (`.tar.gz` file)
3. Figure out what other libraries it needs (dependencies)
4. Download those libraries (and their dependencies... and their dependencies...)
5. Compile all the source code yourself (`./configure && make && make install`)
6. Hope it works on your system
7. When you want to uninstall â€” good luck finding every single file it created

**This was a nightmare.** Package managers eliminate this entirely.

**A package manager automatically:**
- Knows **where to download** software from (official repositories)
- Knows **exactly what dependencies** are needed and installs them automatically
- Tracks **every file** installed so removal is perfectly clean
- Knows **what version** you have and what's available as an update
- Handles **cryptographic verification** so you know the software is legitimate

### 9.2 How APT Works Internally

**APT** (Advanced Package Tool) is used on Ubuntu and Debian. Let's understand what actually happens behind each command.

#### The Repository System

```bash
# See your configured repositories
cat /etc/apt/sources.list
ls /etc/apt/sources.list.d/    # Additional repo files
```

A repository entry looks like this:
```
deb http://archive.ubuntu.com/ubuntu jammy main restricted universe multiverse
```

Breaking this down piece by piece:
- `deb` â†’ binary packages (pre-compiled, ready to install â€” not source code)
- `http://archive.ubuntu.com/ubuntu` â†’ the URL of the server hosting packages
- `jammy` â†’ Ubuntu release codename (22.04 is called "Jammy Jellyfish")
- `main restricted universe multiverse` â†’ the categories of software to include

**Software categories explained:**

| Category | Meaning |
|---|---|
| `main` | Officially supported by Ubuntu, open source software |
| `restricted` | Officially supported by Ubuntu, but NOT open source (e.g., GPU drivers) |
| `universe` | Community maintained, open source, not officially supported |
| `multiverse` | Not free/open source software (e.g., some codecs) |

#### What `apt update` Actually Does

```bash
sudo apt update
```

APT connects to **each repository server** in your sources list and downloads an **index file** â€” a giant catalog listing every available package, its version, description, size, dependencies, and download URL. This index gets stored locally at `/var/lib/apt/lists/`.

> âš ï¸ **CRITICAL to understand:** `apt update` **does NOT install or change anything**. It only refreshes your local catalog of what's available. It's like downloading the latest version of a shopping catalog â€” you're just updating the catalog, not buying anything yet.

```bash
# See the local package index files that apt update downloads
ls /var/lib/apt/lists/

# These files contain the complete catalog of available packages
# apt searches through these local files (fast!) instead of hitting the internet each time
```

#### What `apt install nginx` Actually Does

```bash
sudo apt install nginx
```

Here's the full sequence:

1. **APT checks your local index** for a package named `nginx`
2. **Reads the dependency list** â€” nginx requires: `nginx-common`, `libpcre3`, `zlib1g`, etc.
3. **Calculates the full dependency tree** â€” each dependency might have its own dependencies
4. **Shows you the plan** â€” "The following NEW packages will be installed: ..."
5. **Downloads all required `.deb` packages** to `/var/cache/apt/archives/`
6. **Verifies cryptographic signatures** â€” ensures packages haven't been tampered with
7. **Unpacks each `.deb` package**
8. **Runs pre-installation scripts** (if any)
9. **Copies files to their final system locations** (`/usr/sbin/nginx`, `/etc/nginx/`, etc.)
10. **Runs post-installation scripts** (often starts the service automatically)
11. **Updates the local database** of installed packages at `/var/lib/dpkg/info/`

```bash
# After installation, explore what happened:

# See every single file that nginx installed
dpkg -L nginx
# Output: /etc/nginx, /etc/nginx/nginx.conf, /usr/sbin/nginx, ...

# Find which package installed a specific file
dpkg -S /usr/sbin/nginx
# Output: nginx: /usr/sbin/nginx

# See detailed information about a package
apt show nginx

# Verify nginx is installed and see its version
dpkg -l | grep nginx

# See what nginx depends on
apt-cache depends nginx
```

---

### 9.3 APT Commands â€” Complete Reference

```bash
# â•â• INFORMATION â€” No sudo needed, read-only operations â•â•â•â•â•â•â•â•â•â•â•â•
apt search nginx              # Search available packages by name/description
apt show nginx                # Show detailed info: version, size, dependencies, description
apt list --installed          # List all installed packages
apt list --upgradable         # List packages that have updates available
apt-cache depends nginx       # Show what packages nginx depends on
apt-cache rdepends nginx      # Show what packages depend ON nginx

# â•â• UPDATING PACKAGE LISTS â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
sudo apt update               # Refresh local package index from repositories
                              # Run this FIRST before anything else

# â•â• UPGRADING INSTALLED SOFTWARE â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
sudo apt upgrade              # Upgrade all installed packages (SAFE â€” won't remove anything)
sudo apt full-upgrade         # Upgrade + remove conflicting packages if needed (more aggressive)
sudo apt upgrade nginx        # Upgrade only one specific package

# â•â• INSTALLING SOFTWARE â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
sudo apt install nginx                # Install nginx
sudo apt install nginx -y             # Install without asking "Do you want to continue? Y/n"
                                      # (-y is essential in scripts â€” no human to press Y)
sudo apt install nginx=1.18.0-0ubuntu1  # Install a SPECIFIC version
sudo apt install --reinstall nginx    # Reinstall (useful if files got corrupted)
sudo apt install nginx curl git -y    # Install multiple packages at once

# â•â• REMOVING SOFTWARE â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
sudo apt remove nginx         # Uninstall nginx but KEEP configuration files
                              # (so if you reinstall, your config is still there)
sudo apt purge nginx          # Uninstall AND delete all configuration files
                              # (completely clean removal)
sudo apt autoremove           # Remove packages that were installed as dependencies
                              # but are no longer needed by anything
                              # (Run this regularly to keep disk clean)
sudo apt autoclean            # Delete old package files from /var/cache/apt/archives/
                              # (keeps only packages that can still be downloaded)
sudo apt clean                # Delete ALL downloaded package files from cache
                              # (frees disk space, packages must be re-downloaded if needed)

# â•â• THE GOLDEN WORKFLOW â€” Always do this â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
sudo apt update && sudo apt upgrade -y   # Update catalog AND install updates
sudo apt install nginx -y               # Then install what you need
sudo apt autoremove -y                  # Clean up periodically
```

---

### 9.4 DNF / YUM â€” For RHEL, Amazon Linux, Fedora

`yum` (Yellowdog Updater Modified) is the older tool. `dnf` (Dandified YUM) is the modern replacement. Their commands are nearly identical â€” just swap one for the other.

```bash
# â•â• INFORMATION â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
dnf search nginx              # Search for packages
dnf info nginx                # Detailed package information
dnf list installed            # All installed packages
dnf check-update              # List packages with available updates

# â•â• INSTALLING / UPDATING / REMOVING â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
sudo dnf install nginx        # Install
sudo dnf install nginx -y     # Install without confirmation
sudo dnf remove nginx         # Remove
sudo dnf update               # Update ALL packages
sudo dnf update nginx         # Update only nginx

# â•â• DNF'S KILLER FEATURE â€” TRANSACTION HISTORY â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# This is something APT can't do natively!
dnf history                   # Show all past dnf transactions (numbered list)
dnf history info 5            # Show details of transaction number 5
sudo dnf history undo 5       # UNDO transaction 5 â€” rolls back that install/update!
# This is invaluable when an update breaks something in production
```

**Other package managers:**

```bash
# â•â• PACMAN (Arch Linux) â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
sudo pacman -Syu              # Sync repos AND upgrade all packages
sudo pacman -S nginx          # Install nginx
sudo pacman -R nginx          # Remove nginx
sudo pacman -Ss nginx         # Search for nginx
sudo pacman -Qi nginx         # Info about installed package

# â•â• ZYPPER (OpenSUSE) â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
sudo zypper refresh           # Refresh package lists (like apt update)
sudo zypper update            # Update all packages
sudo zypper install nginx     # Install
sudo zypper remove nginx      # Remove
sudo zypper search nginx      # Search
```

---

### 9.5 Adding New Repositories

The default repositories don't have everything. Sometimes you need to add a third-party repository for newer versions or specialized software.

```bash
# â•â• UBUNTU/DEBIAN â€” Adding a PPA (Personal Package Archive) â•â•â•â•â•â•â•
# PPAs are individual developer repositories hosted on Launchpad

sudo add-apt-repository ppa:nginx/stable    # Add the nginx stable PPA
sudo apt update                             # Refresh â€” now sees new repo's packages
sudo apt install nginx                      # Install from the PPA

# â•â• MANUALLY ADDING A REPOSITORY â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 1: Add the repository source file
echo "deb [signed-by=/usr/share/keyrings/example.gpg] https://packages.example.com/ubuntu jammy main" | \
    sudo tee /etc/apt/sources.list.d/example.list

# Step 2: Add the repository's GPG signing key
# This is a SECURITY step â€” proves the packages are from the real publisher
curl -fsSL https://packages.example.com/gpg | \
    sudo gpg --dearmor -o /usr/share/keyrings/example.gpg

# Step 3: Update and install
sudo apt update
sudo apt install example-package

# â•â• WHY THE GPG KEY MATTERS â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Every package in a repository is cryptographically signed with the
# repository owner's private key. APT verifies this signature using the
# public key you add. If someone intercepts your download and replaces
# the package with malware, the signature won't match and APT REFUSES
# to install it. This is why you should only add keys from trusted sources.
```

---

## 10. Linux vs. Windows

Don't think of this as "Linux is better" â€” think of it as "Linux is better **for servers and DevOps work**, and here's the specific reason for each point":

| Feature | Linux | Windows | Why It Matters for DevOps |
|---|---|---|---|
| **Cost** | Free | Paid licenses | At 10,000 servers, Linux saves millions in licensing per year |
| **Source** | Open Source | Proprietary | Open source = inspect code, find bugs, contribute fixes. Full transparency |
| **File System** | Hierarchical (`/`) | Drive Letters (`C:`, `D:`) | Consistent paths make scripting and automation reliable |
| **Case Sensitivity** | `File.txt` â‰  `file.txt` | Case-insensitive | Prevents ambiguity bugs in CI/CD scripts |
| **Multi-user** | Built for multiple users | Single-user focus | Secure, stable shared server operations |
| **Package Mgmt** | `apt`, `yum`, etc. | Download `.exe` files manually | Package managers automate dependency management at scale |
| **Resource Use** | Very lightweight | Resource-heavy | Better CPU/RAM efficiency for containers and thousands of servers |
| **Stability** | Servers run for YEARS without rebooting | Requires frequent reboots for updates | Ensures 99.99%+ uptime in production environments |
| **Security** | Strong privilege model, open code scrutiny | More frequent vulnerabilities | Fewer malware risks, faster transparent patches |
| **Automation** | Everything is a text file or command | Registry, GUIs required | Bash + cron + text configs = easily scriptable everything |
| **Container Support** | Docker was built FOR Linux | Workaround via WSL2 | Native container performance, essential for Kubernetes |

> **Where Windows still wins:** Desktop experience for non-technical users, some enterprise software (Active Directory, legacy .NET apps, certain industry-specific tools).

---

## 11. The GNU Project & Open Source Philosophy

### The Surprising Fact That Changes How You See Linux

> **Linus Torvalds did NOT write `ls`, `bash`, `grep`, `cp`, `cat`, `mkdir`, or most of the commands you use every day.**

Those came from the **GNU Project**, started by **Richard Stallman in 1983** â€” **8 years before Linux existed.**

Stallman's GNU Project was building a complete free Unix-like OS but had one missing piece: **a kernel**. Linux had a kernel but lacked the userland tools needed to do anything useful with it. They combined in the early 1990s, and the result is what we use today â€” which is technically **GNU/Linux**.

> **Simple way to remember it:** "The kernel is the engine. GNU tools are the steering wheel, pedals, and dashboard. Linus built the engine. GNU built everything else you touch. You need both to have a working car."

### The GPL â€” Why Linux Stays Free

The Linux kernel is licensed under the **GNU General Public License (GPL)**. This license has one critical rule:

> *If you distribute software that uses or is derived from GPL-licensed code, you must also distribute the source code under the same GPL license.*

This means companies that use Linux (like Red Hat, Google, AWS) must contribute their changes back to the community. No one can take Linux, make it proprietary, and lock it away. This is what drives the collaborative innovation.

### The Open Source Philosophy

Linux is built on three core principles:

1. **Freedom** â€” Anyone can use, study, modify, and distribute the code. No restrictions.
2. **Collaboration** â€” Thousands of developers worldwide contribute. Problems get found and fixed faster than any single company could manage.
3. **Transparency** â€” The code is open. You can see exactly what it does. Security issues are found (and fixed) faster because thousands of eyes inspect the code.

> **Why do Google, AWS, and Red Hat invest millions in Linux?** Because they build their entire business on it. AWS runs its EC2 instances on Linux. Google's data centers run Linux. Making Linux better directly benefits their business â€” and the GPL ensures their contributions also benefit everyone else.

---

## 12. Common Beginner Mistakes

### âŒ Mistake 1: Running `apt install` Without `apt update` First

```bash
# WRONG â€” You might install an outdated, buggy, or incompatible version
# because your local package index is stale
sudo apt install nginx

# RIGHT â€” Always refresh the catalog first so you get the latest version
sudo apt update && sudo apt install nginx
```

### âŒ Mistake 2: Confusing `apt update` and `apt upgrade`

This confuses nearly every beginner:

```bash
sudo apt update    # Refreshes your LOCAL CATALOG of what's available
                   # Downloads index files only â€” NO software is changed
                   # Think: "update the shopping catalog"

sudo apt upgrade   # Actually INSTALLS newer versions of installed software
                   # Think: "buy the items that have newer versions"
```

Running `apt update` without `apt upgrade` afterward means you updated the catalog but installed nothing. Running `apt upgrade` without `apt update` first means you're upgrading based on a possibly outdated catalog.

### âŒ Mistake 3: Using the Wrong Package Manager for Your Distro

```bash
# On Amazon Linux â€” apt does NOT exist
sudo apt install nginx    # bash: apt: command not found

# On Ubuntu â€” yum does NOT exist
sudo yum install nginx    # bash: yum: command not found

# Always check your distro FIRST:
cat /etc/os-release
# Then use the correct package manager:
# Ubuntu/Debian â†’ apt
# RHEL/Amazon Linux/Fedora â†’ dnf or yum
# Arch â†’ pacman
```

### âŒ Mistake 4: Forgetting `sudo`

```bash
apt install nginx        # Error: E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)

sudo apt install nginx   # Works â€” sudo runs the command with root (administrator) privileges
```

Most system-level operations require root privileges. `sudo` (Super User Do) temporarily grants those privileges for one command. Without it, the kernel rejects the request.

### âŒ Mistake 5: Thinking the Kernel IS the Distribution

The kernel is **one component** of a distribution. When you say "I use Ubuntu," you're referring to the complete distribution: kernel + GNU tools + apt + default software + configuration + everything bundled together.

Two people can both be running Linux kernel version `5.15.0` â€” one on Ubuntu, one on Arch Linux â€” and have completely different commands, file locations, and package managers.

### âŒ Mistake 6: Not Cleaning Up After Removing Packages

```bash
# When you install package A, it automatically installs dependencies B and C
sudo apt install packageA    # Installs A, B, and C

# When you remove A, B and C are left behind â€” they're now "orphans"
sudo apt remove packageA     # Only removes A

# You must explicitly clean up the orphaned dependencies
sudo apt autoremove          # Removes B and C (and any other orphans)

# For a completely clean removal:
sudo apt purge packageA      # Remove A AND delete its config files
sudo apt autoremove          # Remove orphaned dependencies
```

### âŒ Mistake 7: Thinking an App Crash Crashes the Whole System

Because of kernel space / user space separation, a crashed application stays **isolated in user space**. The kernel keeps running. Other processes keep running. Only a kernel bug (called a **kernel panic** â€” very rare in production) crashes the whole system.

This is fundamentally different from how older, poorly-designed operating systems worked.

### âŒ Mistake 8: Running Everything as Root

```bash
# Dangerous â€” doing normal work as root
sudo bash          # Opens a root shell â€” every command runs as administrator
rm -rf /home/user  # One typo and you've deleted everything

# Better â€” use sudo only for specific commands that need it
sudo apt install nginx     # Just this one command runs as root
ls /home/user              # This runs as your normal user (safe)
```

Running as root bypasses all permission checks. One typo or compromised script can destroy your entire system. Use `sudo` for specific commands, not as a way of life.

---

## 13. Hands-On Tasks

### Task 1: Identify Your System

```bash
# What distro am I on? What version?
cat /etc/os-release

# What is my kernel version?
uname -r
# Example output: 5.15.0-91-generic
# Format: major.minor.patch-build-flavour

# All system info at once
uname -a
# Output: Linux hostname 5.15.0-91-generic #101-Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
#         ^OS  ^hostname  ^kernel version                                                    ^architecture
```

### Task 2: Explore Running Processes

```bash
# See all running processes
ps aux | head -20

# See PIDs as directories â€” the kernel's own view of processes
ls /proc/ | head -20        # Each number = one running process

# Read info about YOUR current bash session
cat /proc/$$/status
# $$ = the PID of your current shell process

# What's using the most CPU right now?
ps aux --sort=-%cpu | head -10

# What's using the most memory?
ps aux --sort=-%mem | head -10
```

### Task 3: Explore Memory

```bash
# Human-readable memory overview
free -h

# Read memory info directly from the kernel's virtual filesystem
cat /proc/meminfo | grep -E "MemTotal|MemAvailable|SwapTotal|SwapFree"

# See file descriptors of your current shell process
ls /proc/self/fd
# fd 0 = stdin, fd 1 = stdout, fd 2 = stderr, others = open files
```

### Task 4: Watch System Calls Happen Live

```bash
# Install strace (system call tracer)
sudo apt install strace

# See every system call ls makes â€” this shows the kernel/user space boundary in action
strace ls /tmp 2>&1 | less
# Press q to quit less

# Summary mode â€” count how many times each system call was made
strace -c ls /tmp

# Trace a longer-running process (Ctrl+C to stop)
strace -c sleep 5
```

### Task 5: Practice the Full Package Manager Workflow

```bash
# Step 1: Always update first
sudo apt update

# Step 2: Install a useful tool
sudo apt install tree

# Step 3: Use it
tree /etc -L 1        # Show /etc directory as a visual tree, 1 level deep
tree /var/log -L 2    # Show /var/log tree, 2 levels deep

# Step 4: Get info about what we installed
dpkg -L tree          # Every file tree installed
apt show tree         # Package metadata

# Step 5: Remove it cleanly
sudo apt remove tree
sudo apt autoremove   # Clean up any orphaned dependencies
```

### Task 6: Write Your First System Info Script

```bash
# Create the script file
nano system_info.sh
```

Type this content into the file:

```bash
#!/bin/bash
# The #!/bin/bash line is called a "shebang" â€” tells the OS to use bash to run this script

echo "========================================="
echo "          SYSTEM INFORMATION             "
echo "========================================="
echo ""
echo "Hostname:     $(hostname)"
echo "Current User: $(whoami)"
echo "Home Dir:     $HOME"
echo "Shell:        $SHELL"
echo "Directory:    $(pwd)"
echo ""
echo "--- Kernel & OS ---"
echo "Kernel:       $(uname -r)"
echo "Architecture: $(uname -m)"
echo "Distribution:"
cat /etc/os-release | grep PRETTY_NAME | cut -d'"' -f2
echo ""
echo "--- Memory ---"
free -h | grep Mem
echo ""
echo "--- Disk Usage ---"
df -h / | tail -1
echo ""
echo "--- Uptime ---"
uptime
echo "========================================="
```

Save with `Ctrl+O`, press `Enter`, then exit with `Ctrl+X`.

```bash
# Make it executable
chmod +x system_info.sh
# chmod = change file mode
# +x    = add execute permission

# Run it
./system_info.sh
# ./ means "look in the current directory for this file"
# (bash doesn't look in the current directory for safety reasons â€” you must specify ./)
```

### Task 7: Practice Standard Streams and Pipes

```bash
# Redirect stdout to a file
ls /home > my_files.txt
cat my_files.txt                    # Verify it worked

# Append more output to the same file
date >> my_files.txt                # Appends current date
cat my_files.txt                    # Now has both ls output AND date

# Redirect stderr to a separate file
ls /nonexistent_folder 2> errors.txt
cat errors.txt                      # Contains the error message

# Discard stderr so errors don't clutter your screen
find / -name "*.conf" 2>/dev/null   # Find all .conf files, ignore permission errors

# Chain commands with pipes
ps aux | grep bash | wc -l          # How many bash processes are running?
cat /etc/passwd | cut -d: -f1       # Extract just usernames from passwd file
ls -lt /var/log | head -5           # 5 most recently modified log files
```

### Task 8: Explore the Network

```bash
# Your IP addresses and network interfaces
ip addr show

# Active connections and what's listening
ss -tulpn

# Your routing table
ip route show

# Test network connectivity
ping -c 4 google.com    # Send 4 pings to google.com (-c = count)
```

---

## 14. Quick Revision Summary

### The Kernel (5 Key Points)
1. Core software managing **CPU, RAM, disk, network** â€” created by Linus Torvalds in 1991
2. Five jobs: **Process management, Memory management, File system, Device drivers, Networking**
3. Never touched directly by applications â€” only via **system calls**
4. Version format: `X.Y.Z` â€” check with `uname -r`
5. Manages both **user space isolation** AND **hardware interaction**

### Kernel Space vs. User Space (5 Key Points)
1. **Kernel Space (Ring 0):** Full hardware access â€” where kernel operates
2. **User Space (Ring 3):** Restricted â€” where ALL applications run
3. **System calls:** The only legal bridge between them
4. **CPU hardware** enforces this separation (not just software rules)
5. Containers exploit this: share the host kernel, isolated by **cgroups** (resources) and **namespaces** (visibility)

### The Shell (5 Key Points)
1. Command interpreter â€” translates human commands to kernel system calls
2. **Bash** is the DevOps standard â€” learn this first
3. Command flow: **parse â†’ check type â†’ find in PATH â†’ fork â†’ execve â†’ run â†’ exit**
4. **Three streams:** stdin (0/keyboard), stdout (1/screen), stderr (2/screen)
5. **Pipe `|`** chains commands: stdout of left â†’ stdin of right

### Linux Distributions (3 Key Points)
1. Kernel + GNU tools + package manager + apps = **Distribution**
2. **Debian family** (Ubuntu, Kali) â†’ uses `apt` â†’ `.deb` packages
3. **Red Hat family** (RHEL, Amazon Linux, Fedora) â†’ uses `dnf`/`yum` â†’ `.rpm` packages

### Package Managers (5 Key Commands)
```bash
sudo apt update              # Refresh local package catalog (no installs!)
sudo apt upgrade             # Install available updates
sudo apt install X           # Install package X with all dependencies
sudo apt remove X            # Remove X (keep config files)
sudo apt purge X             # Remove X + delete config files
sudo apt autoremove          # Clean up orphaned dependencies
```

**The golden rule:** Always `sudo apt update && sudo apt upgrade -y` before installing anything.

### Essential Commands to Memorize
```bash
uname -r              # Kernel version
cat /etc/os-release   # What distro am I on?
ps aux                # All running processes
ps aux --sort=-%cpu   # Sorted by CPU usage
free -h               # Memory usage
df -h                 # Disk usage
ip addr show          # Network interfaces and IPs
ss -tulpn             # Listening ports and active connections
type X                # Find out what a command is (alias/builtin/program)
which X               # Find the full path of an external command
chmod +x script.sh    # Make a script executable
./script.sh           # Run a script from current directory
strace ls             # Watch system calls a command makes
```

### If Someone Asks You to Explain Linux in 30 Seconds

> *"Linux is an open-source operating system kernel created by Linus Torvalds in 1991. A kernel manages hardware resources â€” CPU, memory, storage, and networking. When you combine the kernel with GNU tools and a package manager, you get a complete Linux distribution like Ubuntu or Amazon Linux. It powers over 96% of web servers, all top 500 supercomputers, and nearly every cloud platform. For DevOps engineers, it's the foundation that everything else is built on."*

---

## 15. Interview Preparation Q&A

These are real questions asked in DevOps, SRE, and Cloud Engineering interviews:

---

**Q: What is the Linux kernel, and how does it differ from a Linux distribution?**

A: The Linux kernel is the core software that manages hardware resources â€” it handles process scheduling, memory allocation, device communication, file systems, and networking. It's just one layer. A Linux distribution bundles the kernel with GNU userland tools (bash, ls, grep), a package manager (apt, dnf), and additional software to form a complete, usable operating system. Ubuntu and Amazon Linux both use the Linux kernel, but they're completely different distributions with different tools, package managers, and target use cases.

---

**Q: Explain kernel space vs. user space. Why is this separation important for security and stability?**

A: The CPU operates in privilege rings â€” the kernel runs in Ring 0 (kernel space) with unrestricted hardware access. All applications run in Ring 3 (user space) and cannot directly access hardware. When an app needs a kernel service (like reading a file), it makes a system call â€” the CPU temporarily switches to kernel mode, the kernel performs the operation, then switches back. This separation means a crashing application stays isolated in user space and can't corrupt the kernel or other processes. For DevOps, this is why containers work: they share the host kernel but are isolated in user space using cgroups and namespaces.

---

**Q: Name three Linux distributions and their use cases in cloud environments.**

A: Ubuntu Server â€” the most popular for general cloud VMs, great community support and documentation, 5-year LTS support. Alpine Linux â€” extremely minimal (~5MB), ideal as a Docker container base image because of its tiny size, widely used in Kubernetes workloads. Amazon Linux 2 â€” AWS's optimized distribution for EC2, pre-configured for AWS services, used when you want the best AWS integration out of the box.

---

**Q: How do package managers like apt and dnf work? Why are they essential for DevOps?**

A: Package managers maintain a list of remote software repositories. When you run `apt update`, it downloads index files from those repos â€” a catalog of every available package, version, and dependency. When you install a package, it reads the dependency tree, downloads all required packages, verifies their cryptographic signatures, and installs them in the correct order. They're essential for DevOps because they enable reproducible, automated software management across hundreds of servers without manual intervention, ensure dependency conflicts are caught before installation, and allow rollbacks.

---

**Q: What is the GNU Project and why is the full name "GNU/Linux"?**

A: The GNU Project was started by Richard Stallman in 1983 to build a free Unix-like OS. By 1991 it had almost everything except a working kernel. Linux provided the kernel, GNU provided the userland tools (bash, ls, grep, cp, gcc). Together they form a complete OS â€” GNU/Linux. Linus Torvalds wrote the kernel; he did NOT write the commands we use daily.

---

**Q: Walk me through what happens when you type `ls` and press Enter.**

A: Bash receives the input and parses it. It checks if `ls` is an alias, function, builtin, or external program â€” finds it's aliased to `ls --color=auto`, resolves to `/usr/bin/ls`. Bash calls `fork()` creating a child process. The child calls `execve("/usr/bin/ls")` â€” the kernel loads the ls program, replacing the child process's memory. ls runs in user space, making system calls: `openat()` to open the directory, `getdents64()` to read entries, `stat()` for file metadata, `write()` to output results to stdout. ls calls `exit(0)`. The kernel destroys the child process, bash's `wait()` returns, bash shows the prompt.

---

**Q: Why do containers start faster than VMs?**

A: A VM must boot a complete guest operating system â€” BIOS/UEFI, bootloader, kernel initialization, init system, all services. This takes seconds to minutes. A container is just a process running in user space on the host â€” it shares the host kernel (already running), so starting a container means starting one process with isolation applied via cgroups and namespaces. This takes milliseconds. There's no duplicate kernel, no hypervisor overhead, no OS boot sequence.

---

**Q: How would you check the kernel version, and why might AWS use older kernel versions on EC2?**

A: Check with `uname -r`. AWS uses older, well-tested kernel versions for several reasons: stability (a kernel version that's been in production for years has known behavior and few surprises), compatibility (older versions are guaranteed to work with existing customer workloads and drivers), and support contracts (RHEL and Ubuntu LTS kernels have enterprise support with security patches backported to stable versions without disruptive feature changes).

---

## 16. Glossary

| Term | Definition |
|---|---|
| **Kernel** | The core OS software managing hardware resources (CPU, RAM, disk, network). The Linux kernel was created by Linus Torvalds in 1991. |
| **Distribution (Distro)** | A complete OS: Linux kernel + GNU tools + package manager + additional software. Examples: Ubuntu, Amazon Linux, Alpine. |
| **Shell** | A command-line interpreter. You type human-readable commands; the shell translates them to kernel system calls. Bash is the most common. |
| **Bash** | Bourne Again Shell â€” the default shell on most Linux systems, the standard for DevOps scripting. |
| **Process** | A running instance of a program. Has its own PID, memory space, and state (running/sleeping/stopped/zombie). |
| **PID** | Process ID â€” a unique number the kernel assigns to each running process. PID 1 is always systemd. |
| **fork()** | System call that creates an exact copy (child process) of the current process. The basis of how all new processes are created. |
| **execve()** | System call that replaces the current process's memory with a new program. Used with fork() to run commands. |
| **System Call** | The only bridge between user space and kernel space. Applications request kernel services via system calls. |
| **Kernel Space** | CPU Ring 0 â€” privileged mode where the kernel operates with full hardware access. |
| **User Space** | CPU Ring 3 â€” restricted mode where all applications run. Cannot directly access hardware. |
| **CPU Rings** | Hardware-enforced privilege levels (Ring 0 = kernel/full access, Ring 3 = user/restricted). |
| **cgroups** | Control Groups â€” kernel feature that limits and monitors resource usage (CPU, RAM, disk I/O) per container or process group. |
| **namespaces** | Kernel feature that isolates what a process can see: other processes, network interfaces, filesystem, users. Used by containers. |
| **Package Manager** | Tool automating software install/update/remove. Manages dependencies automatically. `apt` for Debian/Ubuntu, `dnf` for RHEL/Amazon Linux. |
| **Repository (Repo)** | Online server storing software packages. APT downloads packages from repos listed in `/etc/apt/sources.list`. |
| **GNU Project** | Started 1983 by Richard Stallman. Provides userland tools (bash, ls, grep, cp) that make the kernel usable. |
| **GNU/Linux** | The technically correct name â€” GNU tools + Linux kernel = complete OS. |
| **GPL** | GNU General Public License â€” Linux's license. Derivative works must also be open source. Keeps Linux free forever. |
| **Open Source** | Software with freely available, modifiable, distributable source code. |
| **stdin** | Standard Input â€” stream 0. Default: keyboard. Programs read their input from here. |
| **stdout** | Standard Output â€” stream 1. Default: terminal screen. Programs write normal output here. |
| **stderr** | Standard Error â€” stream 2. Default: terminal screen. Programs write error messages here (separate from stdout). |
| **Pipe (`\|`)** | Connects stdout of one command to stdin of the next. Chains simple tools into powerful one-liners. |
| **Redirection** | `>` sends stdout to a file. `2>` sends stderr to a file. `>>` appends instead of overwriting. |
| **PATH** | Environment variable listing directories where bash looks for external commands (e.g., `/usr/bin:/bin`). |
| **Virtual Memory** | Kernel abstraction giving each process the illusion it has all RAM to itself. The kernel maps virtual to physical addresses. |
| **Swap** | Disk space used as overflow when RAM fills up. Much slower than RAM but prevents out-of-memory crashes. |
| **Device Driver** | Code (usually a kernel module) that lets the kernel communicate with specific hardware using its native protocol. |
| **systemd** | The modern init system â€” PID 1. Starts all services at boot, manages services during runtime. |
| **GRUB** | Grand Unified Bootloader â€” loads the kernel from disk into RAM at boot time. |
| **Init System** | The first process started by the kernel (PID 1). Responsible for starting all other services. |
| **Alpine Linux** | Minimal Linux distribution (~5MB) popular as a Docker base image due to tiny size. |
| **GPG Key** | Cryptographic key used to verify software packages are authentic and unmodified. Required when adding third-party repos. |
| **Kernel Panic** | Linux equivalent of Windows "Blue Screen of Death" â€” a fatal kernel error. Extremely rare in production. |
| **LTS** | Long Term Support â€” a distribution release with extended security update support (Ubuntu LTS = 5 years). |
| **dmesg** | Command that displays kernel ring buffer messages â€” useful for diagnosing hardware and driver issues. |
| **strace** | Tool that intercepts and displays system calls made by a process. Shows the user space / kernel space boundary in action. |
| **Shebang (`#!/bin/bash`)** | First line of a script telling the OS which interpreter to use. `#!` = shebang, `/bin/bash` = use bash. |
| **chmod** | Change file permissions. `chmod +x script.sh` adds execute permission, making it runnable. |
| **sudo** | Super User Do â€” runs a command with root (administrator) privileges for that one command only. |

---

## ğŸ“– Resources for Further Learning

- **Official Ubuntu Documentation:** https://ubuntu.com/tutorials
- **The Linux Command Line (free book):** https://linuxcommand.org/tlcl.php
- **Linux Kernel Documentation:** https://www.kernel.org/doc/html/latest/
- **Red Hat Learning:** https://www.redhat.com/en/services/training